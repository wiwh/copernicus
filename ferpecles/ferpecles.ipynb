{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delineation of Ferpecles Glacier Contours Across time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FerpÃ¨cles glacier is melting.\n",
    "\n",
    "We use the Copernicus' Sentinel 2 data to denileat the glaciers and estimate its surface. Follow along as we authenticate to the server and download the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import json\n",
    "from ipyleaflet import Map, GeoJSON, basemaps, Rectangle\n",
    "\n",
    "from sentinelhub import (\n",
    "    SHConfig,\n",
    "    Geometry,\n",
    "    DataCollection,\n",
    "    MimeType,\n",
    "    SentinelHubDownloadClient,\n",
    "    SentinelHubRequest,\n",
    "    bbox_to_dimensions,\n",
    "    MosaickingOrder,\n",
    "    BBox,\n",
    "    CRS,\n",
    "    bbox_to_dimensions\n",
    ")\n",
    "DATA_PATH  = \"../data/ferpecles/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to Copernicus\n",
    "A connection to the Copernicus servers is necessary to request new data. Copernicus uses Oauth2 to authenticate to the servers; it requires a token, which is received after authentication with a `client_id` and `client_secret`. To obtain these latter follow the documentation [here](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Overview/Authentication.html). The `SHConfig` class of `sentinelhub` manages this under the hood. The `client_id` and `client_secret`, are typically stored as a profile in a local `config.toml` file.\n",
    "\n",
    "If the data has been previously obtained and is in the data folder, the `profile` argument of `SHConfig` can be ommitted as no connection to the server will be established in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SHConfig(profile=\"copernicus\") # loads my profile to download data, change for your own profile. The `config.toml` file is in config.get_config_location()\n",
    "# config = SHConfig()  # If data is already in DATA_PATH, no profile is needed and you can replace the above line by this\n",
    "config.sh_token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\n",
    "config.sh_base_url = \"https://sh.dataspace.copernicus.eu\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the contours of the area of interest\n",
    "We can define the contours of the area of interest by using a shape defined in the `ferpecles.geojson` file. Custom `geojson` files can easily be created on (geojson.io).\n",
    "\n",
    "Note: Here, we need to be careful about the units. The units of geometry with WGS84 are degrees. To get a 20m resolution, the geometry must be transformed to EPSG:3035."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GeoJSON into a GeoDataFrame and JSON objects\n",
    "ferpecles_gdf = gpd.read_file(\"ferpecles.geojson\").to_crs(\"EPSG:3035\") # GeoDataFrame (gdf) allows for spatial operations and is useful for analysis. See above note.\n",
    "ferpecles_geometry = Geometry(geometry=ferpecles_gdf.geometry.values[0], crs=ferpecles_gdf.crs) \n",
    "ferpecles_data = json.load(open(\"ferpecles.geojson\", \"r\")) # JSON is often used in APIs an web apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the map and layer\n",
    "To plot the image we need a center and a zoom level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ferpecles_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mmap\u001b[39m \u001b[38;5;241m=\u001b[39m Map(basemap\u001b[38;5;241m=\u001b[39mbasemaps\u001b[38;5;241m.\u001b[39mOpenStreetMap\u001b[38;5;241m.\u001b[39mMapnik, center\u001b[38;5;241m=\u001b[39mcenter, zoom\u001b[38;5;241m=\u001b[39mzoom)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Add the GeoDataFrame as a GeoJSON layer\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m geo_json_layer \u001b[38;5;241m=\u001b[39m GeoJSON(data\u001b[38;5;241m=\u001b[39m\u001b[43mferpecles_json\u001b[49m, style\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopacity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m})\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mmap\u001b[39m\u001b[38;5;241m.\u001b[39madd_layer(geo_json_layer)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Display the map\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ferpecles_json' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute the centroid\n",
    "center = [46.02, 7.55] # [latitude, longitude]\n",
    "zoom = 12\n",
    "map = Map(basemap=basemaps.OpenStreetMap.Mapnik, center=center, zoom=zoom)\n",
    "\n",
    "\n",
    "# Add the GeoDataFrame as a GeoJSON layer\n",
    "\n",
    "geo_json_layer = GeoJSON(data=ferpecles_json, style={\"color\": \"blue\", \"opacity\": 0.7, \"weight\": 2})\n",
    "map.add_layer(geo_json_layer)\n",
    "\n",
    "# Display the map\n",
    "map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requesting Sentinel-2 Data\n",
    "Here we request all bands, and we additionally add scene classification (SCL). Once the request is defined, it can be used to get the actual data. It will look in the `data_folder` to get the data from there; if it doesn't exist it will use the `config` object to access the servers and download it.\n",
    "\n",
    "### 1. Define the create_request function to create our custom request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write the evalscript refer to [the docs](https://docs.sentinel-hub.com/api/latest/evalscript/v3/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_request(geometry, time_interval, config, data_folder):\n",
    "    \"\"\"\n",
    "    Creates a Sentinel Hub request for satellite imagery data.\n",
    "\n",
    "    Parameters:\n",
    "    - geometry (Geometry): The area of interest as a geometry object.\n",
    "    - time_interval (tuple): Time range for the data request, formatted as ('YYYY-MM-DD', 'YYYY-MM-DD').\n",
    "    - config (SHConfig): Configuration object for Sentinel Hub, including API keys and other settings.\n",
    "    - data_folder (str): File path for storing the downloaded data.\n",
    "\n",
    "    Returns:\n",
    "    - SentinelHubRequest: A configured request object for Sentinel Hub API.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Script executed on Sentinel Hub that defines the output format and content\n",
    "    evalscript = \"\"\"\n",
    "    //VERSION=3\n",
    "    function setup() {\n",
    "        return {\n",
    "            input: [{\n",
    "                bands: [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B11\", \"B12\", \"SCL\", \"dataMask\"]\n",
    "            }],\n",
    "            output: [{\n",
    "                id: \"default\",\n",
    "                bands: 13,\n",
    "                sampleType: \"AUTO\"\n",
    "            }]\n",
    "        };\n",
    "    }\n",
    "\n",
    "    function evaluatePixel(sample) {\n",
    "        return {\n",
    "            default: [sample.B01, sample.B02, sample.B03, sample.B04, sample.B05, sample.B06,\n",
    "                      sample.B07, sample.B08, sample.B8A, sample.B11, sample.B12, sample.SCL, sample.dataMask]\n",
    "        };\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    # Constructing the Sentinel Hub request\n",
    "    request = SentinelHubRequest(\n",
    "        evalscript=evalscript,\n",
    "        input_data=[\n",
    "            SentinelHubRequest.input_data(\n",
    "                data_collection=DataCollection.SENTINEL2_L2A.define_from(\n",
    "                    \"s2l2a\", service_url=config.sh_base_url\n",
    "                ),\n",
    "                time_interval=time_interval,\n",
    "                mosaicking_order=MosaickingOrder.LEAST_CC\n",
    "            )\n",
    "        ],\n",
    "        responses=[\n",
    "            SentinelHubRequest.output_response('default', MimeType.TIFF)\n",
    "        ],\n",
    "        geometry=geometry,\n",
    "        resolution=(20, 20),\n",
    "        config=config,\n",
    "        data_folder=data_folder\n",
    "    )\n",
    "\n",
    "    return request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define time intervals for the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define month intervals \n",
    "start_date = datetime.datetime(2018,6,1)\n",
    "end_date = datetime.datetime.today()\n",
    "\n",
    "current_date = start_date\n",
    "\n",
    "time_intervals = []\n",
    "while current_date < end_date:\n",
    "    # If current month is December, set month_end_date to the start of January next year\n",
    "    if current_date.month == 12:\n",
    "        month_end_date = datetime.datetime(current_date.year + 1, 1, 1)\n",
    "    else:\n",
    "        # Otherwise, set month_end_date to the start of the next month\n",
    "        month_end_date = datetime.datetime(current_date.year, current_date.month + 1, 1)\n",
    "\n",
    "    # Don't append the interval if there's less than 15 days to end_date\n",
    "    if end_date - current_date < datetime.timedelta(15):\n",
    "        break\n",
    "    \n",
    "    time_intervals.append((current_date.isoformat(), month_end_date.isoformat()))\n",
    "    current_date = month_end_date\n",
    "\n",
    "# Print the intervals\n",
    "print(f'A total of {len(time_intervals)} time intervals were created.\\\n",
    "    \\nFirst interval:{time_intervals[0]}\\\n",
    "    \\nLast interval: {time_intervals[-1]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create the requests for all time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_requests = []\n",
    "\n",
    "for time_interval in time_intervals:\n",
    "    request = create_request(geometry=ferpecles_geometry, time_interval=time_interval, config=config, data_folder=DATA_PATH)\n",
    "    all_requests.append(request)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Download the data for all requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all data; get_data() will either download and save the data if none is present in the `DATA_PATH` folder, or it will load it from there.\n",
    "The returned element is a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [request.get_data(save_data=True)[0] for request in all_requests]\n",
    "data = np.stack(data, axis=0)\n",
    "\n",
    "# standardize data by dividing by the max of each band\n",
    "data = data.astype(np.float32) # standardize the imagaes\n",
    "data = data / np.max(data, axis=(0,1,2)).reshape(1,1,1,-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Normalized Difference Snow Inde (NDSI)\n",
    "\n",
    "$$\n",
    "NDSI = \\frac{Green - SWIR}{Green + SWIR}\n",
    "$$\n",
    "\n",
    "The Green channel is B03 (idx 2) and the SWIR channel is B11 (idx 9) or B12 (idx 10)\n",
    "\n",
    "High values of NDSI ( close to 1) indicate the presence of snow or ice. Low values (close to -1) suggests the absence of snow or ice.\n",
    "\n",
    "First, let's check the distribution of the values of the corresponding bands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'image' is a 3D numpy array where the third dimension represents different bands\n",
    "\n",
    "# Plot histogram of the third band\n",
    "plt.hist(data[-1, :, :, 2].flatten(), bins=50, alpha=0.5, label='Band 3')\n",
    "\n",
    "# Plot histogram of the tenth band\n",
    "plt.hist(data[-1, :, :, 9].flatten(), bins=50, alpha=0.5, label='Band 10')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Pixel values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Band 3 (green) and Band 11 (SWIR)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems good enough. Notice that a lot of the zeroes are the unsampled pixels in the image. Let's check the distribution of the NDSI values, to determine an appropriate cutoff. Because of the possible 0 in the denominator, we add a small number in both the numerator and denominator to compute the NDSI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDSI = (data[:, :,:,2] - data[:, :,:,9] + .05)/(data[:,:,:,2] + data[:, :,:,9] + .05)\n",
    "plt.hist(NDSI.flatten(), bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDSI.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cutoff at 0.4 seems appropriate for our purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot image against the NDSI index\n",
    "\n",
    "Here we only consider the months of July."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for demonstration\n",
    "idx = np.arange(1, data.shape[0], 12)  # 2nd month is july\n",
    "\n",
    "fig, axs = plt.subplots(idx.shape[0], 2)\n",
    "fig.set_size_inches(10., 5.*idx.shape[0])\n",
    "\n",
    "\n",
    "# Loop through each image and display its true color representation\n",
    "for i, idx_i in enumerate(idx):\n",
    "    # Extract RGB bands\n",
    "    red_channel = data[idx_i, :, :, 3]   # B04\n",
    "    green_channel = data[idx_i, :, :, 2] # B03\n",
    "    blue_channel = data[idx_i, :, :, 1]  # B02\n",
    "\n",
    "    # Stack them together\n",
    "    true_color_image = np.stack([red_channel, green_channel, blue_channel], axis=-1)\n",
    "\n",
    "    axs[i, 0].imshow(true_color_image)\n",
    "    \n",
    "    # Set title with date information\n",
    "    axs[i, 0].set_title(f\"Image {i + 1} ({time_intervals[idx_i][0]} - {time_intervals[idx_i][1]}) - True Color\")\n",
    "\n",
    "    # Plot NDSI\n",
    "    NDSI_image = NDSI[idx_i]>0.4\n",
    "    mask = data[idx_i, :,:,-1]\n",
    "    NDSI_image[mask<.5] = 0.\n",
    "    axs[i, 1].imshow(NDSI_image)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series of estimated snow surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ice = NDSI > 0.4\n",
    "mask = data[:,:,:,-1] > .5 # True if the pixel is sampled\n",
    "ice[~mask] = 0.\n",
    "\n",
    "surface = np.sum(ice, axis=(1,2)) / np.sum(mask, axis=(1,2))\n",
    "plt.plot(surface)\n",
    "plt.title(\"Proportion of snow coverage across time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we learn that snow melts in summer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
